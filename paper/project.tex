%\documentclass{amsart}

\documentclass{article}
\usepackage[letterpaper,hmargin=15mm,vmargin=20mm]{geometry}
\usepackage[nosetup, colorlinks]{tony}
\usepackage{graphicx}

\usepackage{amsmath,amssymb}
\usepackage{siunitx}

\usepackage{mathpazo}
\usepackage{microtype}
\usepackage{multicol}

\usepackage{diagbox}

\usepackage{color}
\usepackage[dvipsnames]{xcolor}
%\usepackage[printwatermark]{xwatermark}
%\newwatermark*[allpages,color=gray!50,angle=45,scale=3,xpos=0,ypos=0]{DRAFT}

\usepackage{tikz}
\usetikzlibrary{arrows}

\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\NLL}{NLL}
\newcommand{\sind}[1]{^{(#1)}}

\title{Prediction of airline departure delays}
\author{Tony Zhang and Menghua Wu}
\date{December 16, 2016}

\begin{document}
\maketitle

\begin{multicols}{2}

% % % % % % % % % %
%    INTRODUCTION
% % % % % % % % % %

\section{Background}
\label{sec:bg}

Often, people purchase plane tickets
to minimize monetary cost.
However, price is not the only consideration
when consumers purchase tickets---
not all flights are created equal.
In particular,
flyers often care about and take into account
the risk that a flight will be delayed.
This paper hopes to quantify this risk
on the basis of past flight delay data.

The United States Bureau of Transportation Statistics (BTS)
compiles comprehensive datasets annually
regarding the nation's transportation infrastructure,
including aviation, maritime, highway, and rail.
In this paper, we focus on the aviation dataset,
which reports on a wide range of variables concerning individual flights,
including carrier, origin and destination, and flight delays.
We will primarily concern ourselves with classification:
whether a flight is delayed or not.

We downloaded data from June 2015 to May 2016, inclusive.
Each month of data reports on
approximately 500000 individual flights.
Notably, some of the data is missing.
We used these data to build a classifier
that predicts whether or not a flight's departure
is expected to be delayed.
We combined several methods of classification
and compare their results here.

\section{Feature selection and processing}

% TODO describe this

\subsection{Cyclic features}

Features such as time of day and day of year
are naturally cyclic.
Representing them on a linear scale
therefore imposes an unnatural metric on them.
For instance,
if we naively represent times as ``minutes into day",
the time 23:59 will be considered ``more similar"
by any machine learning model to noon than to 00:01,
which disagrees with our intuitive notions of closeness.

A better method is to construct a feature map
that respects our notions of closeness.
An obvious choice is to map a linear data point $x \in [0, T)$
onto a unit circle by
\begin{equation}
    x \mapsto \lt(\cos\f{2\pi x}{T}, \sin\f{2\pi x}{T}\rt).
\end{equation}
In the case of time of day,
noon would have ``angle" $\f{2\pi x}{T} = \pi$
and thus get mapped to the point $(-1, 0)$.

% TODO discuss how this goes

\section{Logistic regression}

To establish a baseline accuracy
and to benchmark our future efforts,
we began with a simple logistic regression classifier.


\section{Random forest classification}

We leveraged existing random forest implementations
and determined the optimal parameters for classifying our data.

There may be strong relationships between
month of year and airline delay,
as the holiday season is often associated with
inclement weather and an increase in delays.
Therefore, we trained a separate random forest classifier
for each month, in additional to a single classifier
for the combined data.
For each month,
the resultant model was an average of
that month's classifier and the combined classifier,
weighted by validation error.

% TODO INSERT PLOT and data

On the testing data,
the combined classifier performed better
than each classifier alone,
perhaps due to increased generalization.

\subsection{Maximum tree depth}

By default, tree depth is unconstrained,
so the classifier runs until all nodes
are expanded into valid leaves.
We experimented with imposing a maximum depth
as a form of early stopping.
This method generally decreased training accuracy,
but increased validation accuracy
for some depths.
%max_depth_leaf 4
%Training data score 0.6570625
%Testing data score 0.6447265113844544
%
%max_depth_leaf 8
%Training data score 0.679025
%Testing data score 0.6563726773096048
%
%max_depth_leaf 16
%Training data score 0.75025
%Testing data score 0.6665794294687255
%
%max_depth_leaf 32
%Training data score 0.8992
%Testing data score 0.6656634388903429
%
%max_depth_leaf 64
%Training data score 0.9726625
%Testing data score 0.6540172729651924
%
%max_depth_leaf 256
%Training data score 0.976025
%Testing data score 0.6472127715257786
\begin{center}
    \begin{tabular}{c|c c c c c}
        max depth
        		& 4 & 8 & 16 & 34 & 256 \\
        training accuracy
        		& 0.657 & 0.679 & 0.750 & 0.899 & 0.976\\
        validation accuracy
        		& 0.644 & 0.656 & 0.667 & 0.654 & 0.647
    \end{tabular}
\end{center}
Imposing a maximum tree depth effectively
reduces overfitting to training data.

\subsection{Number of estimators}

\subsection{Masking features}

xxx maximum number of features to consider when splitting

\subsection{Early stopping}

or the impurity setting

\subsection{Whatever you call ``warm start''}

\section{Dataset transformations}
\label{sec:dataset}

The raw dataset from the BTS
includes unconventional features,
so we preprocessed our dataset
to improve classification feasibility.

Use best parameters from previous section.

\subsection{Discrete features this doesn't go here}

Discrete features in our dataset included
air carrier, airline, origin, and destination.
We mapped these to one-hot vectors,
whose dimensions were flexible based on
the number of categories present in the dataset.


\subsection{Geographic relations within data}

So far, we have considered origin and destination data
as discrete values,
each airport with an equal probability of causing delays.
That is, the distance between any two airports
(as one-hot vectors) is the same.
However, this may not be the case in the real world.

\subsection{Geographic displacement}

East west vs. vice versa, etc.

\section{Comparison with other methods}

We compared our ensemble classifier
with other types of classifiers,
including logistic regression, a neural network,
and a kernelized Pegasos classifier
with the Gaussian RBF kernel.
Due to the large size of our dataset,
we were limited to
stochastic and minibatch methods.

\section{Conclusions}

xxx

\end{multicols}

\end{document}

